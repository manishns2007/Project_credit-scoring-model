
# CREDIT SCORING MODEL PIPELINE

# Install required libraries if running fresh in Colab
!pip install scikit-learn matplotlib seaborn

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve


np.random.seed(42)
n_samples = 1000

data = pd.DataFrame({
    "income": np.random.normal(50000, 15000, n_samples),
    "debt": np.random.normal(15000, 5000, n_samples),
    "payment_history": np.random.choice([0, 1], size=n_samples, p=[0.3, 0.7]),  
    "age": np.random.randint(21, 65, n_samples),
    "credit_score": np.random.randint(300, 850, n_samples)
})

data["creditworthy"] = (
    (data["income"] / (data["debt"] + 1) > 3) & (data["payment_history"] == 1)
).astype(int)

print("Sample Data:")
print(data.head())


X = data.drop("creditworthy", axis=1)
y = data["creditworthy"]


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42, stratify=y
)


models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(max_depth=5, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    results[name] = {
        "Classification Report": classification_report(y_test, y_pred, output_dict=True),
        "ROC-AUC": roc_auc_score(y_test, y_prob)
    }
    
    print(f"\n==== {name} ====")
    print(classification_report(y_test, y_pred))
    print("ROC-AUC:", roc_auc_score(y_test, y_prob))


fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for i, (name, model) in enumerate(models.items()):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", ax=axes[i])
    axes[i].set_title(name)
    axes[i].set_xlabel("Predicted")
    axes[i].set_ylabel("Actual")

plt.show()


plt.figure(figsize=(8,6))

for name, model in models.items():
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc_score(y_test, y_prob):.2f})")

plt.plot([0,1],[0,1],'--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.show()
